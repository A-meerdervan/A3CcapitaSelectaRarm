
Veranderd in run:

	run29oktNoMaxUpdate96
Deze run is bedoeld om te vergelijken met de beste run tot nu toe, die van 26okt met gamma op 96 en buf op 30
en dan te kijken of shit beter leert wanneer er geen update word gedaan wanneer max stappen bereikt word
aangezien dat tot instabiliteit zou kunnen leiden, zie alex zijn logboek
gamma is 96 en de code is dus wat anders wat dat betreft.
	run29oktSparse96
gamma op 96 en Sparse op true. (met echt 100 als je het doel raakt, niet half werk met +40 ofzo)
	run29OktSparse99
gamma op 99 en Sparse op True. (zie boven ... )
	run29oktRandomG99: 
gamma op .99 
de rest zelfde als hieronder
	run29oktRandomG96
gamma op 96
de rest zelfde als hieronder
	run29oktRandomG97
gamma op 97
de rest zelfde als hieronder
	run29oktRandomG95
gamma op 95
de rest zelfde als hieronder
	run29oktRandomG98
gamma op 98
de rest zelfde als hieronder




	
de rest zelfde als hieronder
de rest zelfde als hieronder

# -*- coding: utf-8 -*-
"""
Created on Tue Oct 23 12:28:34 2018

@author: arnol
"""

import numpy as np

ENV_IS_RARM = True

# PARS FOR EVALUATION ONLY
EVAL_MODE = False
EVAL_SHOW_NORMAL_SPEED = True # only relevant during evaluation episodes
EVAL_FPS = 30 # only relevant during evalutation episodes
EVAL_RENDER = False # only relevant ruing evaluation episodes
#EVAL_CPU_CNT = 12 # Number of cpu's used during training
EVAL_FOLDER = 'runRandomG96' # The folder that holds the model and train folders
EVAL_NR_OF_GAMES = 100
# ===================================================================

sim_WINDOW_WIDTH = 400
sim_WINDOW_HEIGHT = 400

sim_RandomGoal = True
sim_SparseRewards = False
sim_Goal = np.array([100,200])
sim_AddNoise = True
sim_goalRadius = 5
sim_GoalReward = 100
sim_expRewardGamma = -0.01 
sim_expRewardOffset = 100
sim_thresholdWall = 20. # linear punishmet starts at this amount of pixels
sim_WallReward = 100
# the normalisation is dependend on whether sparse rewards are used
if sim_SparseRewards:
    # the higest reward must be 1, this ensures that is true.
    sim_rewardNormalisation = max(sim_GoalReward,sim_WallReward)
else: # in case no sparse rewards    
    sim_rewardNormalisation = sim_WallReward + sim_expRewardOffset
print("GlobalCOnst ",sim_rewardNormalisation)



rob_RandomInit = False
rob_NoiseStandDev = 0.001
rob_StepSize = np.radians(1.4)
rob_MaxJointAngle = np.radians(np.array([170,-170]))
rob_JointLenght = [100, 100, 80, 20]
rob_JointWidth = 10
rob_ResetAngles = np.radians(np.array([130,-140,140]))

run_Render = False
run_NumOfWorkers = 12
run_MaxEpisodeLenght = 2000
run_FPS = 15
run_Gamma = .96 # discount rate for advantage estimation and reward discounting
run_sSize = 17 # Observations are greyscale frames of 84 * 84 * 1
run_aSize = 6 # Agent can rotate each joint in 2 directions
run_LoadModel = False
run_LearningRate = 1e-3
run_BufferSize = 30
run_actionSpace = [1,2,3,4,5,6]
run_TFsummIntrvl = 5 # after this many episodes a datapoint is saved
run_TFmodelSaveIntrvl = 250 # after this many episodes the model is saved.


OUTP_FOLDER = '/run29OktSparse96'

OUTP_FOLDER = './LogsOfRuns' + OUTP_FOLDER
TF_SUMM_PATH = OUTP_FOLDER + '/train_'
run_ModelPath = OUTP_FOLDER + '/model'
