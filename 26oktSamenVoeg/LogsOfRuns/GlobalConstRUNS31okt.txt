Alle runs hebben randomWalls op TRUE :D
De gebruikte 7 self.envWalls variabelen heb ik hieronder boven de GlobalConst geplakt

Note dat het random alterneert tussen 7 vaste Envs
Note ook dat er een paar mappen local in de naam hebben. Dat kan er beter uit gehaald worden
achteraf. Ze runden wel degelijk op het cluster. 
	run31oktNachtG98
Pars used, as file below. Was done with the pars of the best run of 29okt. But now
with randomEnv on True :D
	run31oktNachtG98buf20
bufSize to 20
	run31oktNachtG98lr7e
learnR to 7e-4
	run31oktNachtG98lr1eMin4
learnR to 3e-4 NOTE: moet dus de naam veranderen, was perongeluk. wou eigenlijk 1e-4 testen
	run31oktNachtG98ent0.01
entCoef naar 0.01
	run31oktNachtG98Nhid64
nHid nodes naar 64 terwijl het eerst 512 was. Kijken of het uitmaakt in de leertijd/prestaties
	run31oktNachtG98Nhid128
nHid nodes naar 128
	run31oktNachtG98Nhid256
nHid nodes naar 256
	run31oktNachtG98Clip0p5
Nu max clip naar 0.5 vanwege openAI
	run31oktNachtG98ookEnv7
Nu zelfde als de eerste maar dan wel env = 7 meegenomen. Bij alle hierboven gaat het maar tot 6 omdat ik niet 
aan het opletten was. :(

Uit simEnv class:

def setRandomEnv(self):
        # get the random envNr
        envNr = np.random.randint(1, 6 + 1)
        #envNr = 7
        # Switch between environments per reset of the environment.
        # This is a pipe witch a corner to the left (most used during training. Our first env.)
        if envNr == 1:
            self.envWalls = np.array([[(120,400), (120,300)], [(120,300), (40,300)], [(40,300),
                      (40,140)], [(40,140), (280,140)], [(280,140), (280,400)], [(280,400),(120,400)]])
            self.envWallSide = ['l', 'b','l', 't', 'r', 'b']
        # A pipe witch is straight up and wide
        elif envNr == 2:
            rC = 200 # robotCenter, the x pos of the arm bottom
            pR = 80 # the width of the pipe
            self.envWalls = np.array([[(rC-pR,400), (rC-pR,100)], [(rC-pR,100), (rC+pR,100)], [(rC+pR,100),
                      (rC+pR,400)], [(rC+pR,400), (rC-pR,400)]])
            self.envWallSide = ['l', 't','r', 'b']
        # A pipe witch is straight up and narrower
        elif envNr == 3:
            rC = 200 # robotCenter, the x pos of the arm bottom
            pR = 70 # the width of the pipe
            self.envWalls = np.array([[(rC-pR,400), (rC-pR,100)], [(rC-pR,100), (rC+pR,100)], [(rC+pR,100),
                      (rC+pR,400)], [(rC+pR,400), (rC-pR,400)]])
            self.envWallSide = ['l', 't','r', 'b']
        # this is a T shaped pipe 
        elif envNr == 4:
            tYe = 130
            self.envWalls = np.array([[(140,400), (140,300)], [(140,300), (45,300)], [(45,300),
                      (45,tYe)], [(45,tYe), (355,tYe)], [(355,tYe), (355,300)], [(355,300),(260,300)],[(260,300),(260,400)],[(260,400),(140,400)]])
            self.envWallSide = ['l', 'b','l', 't', 'r', 'b','r','b']#            
        # 
        # this is a turn to the right Which is at the top of the reach of the arm
        elif envNr == 5:
            tYs = 200 # rurnYstart This is the start of the right side of the pipe
            tYe = 110 #turn Y end, the top op the pipe
            self.envWalls = np.array([[(140,400), (140,tYe)], [(140,tYe), (355,tYe)], [(355,tYe),
                      (355,tYs)], [(355,tYs),(260,tYs)],[(260,tYs),(260,400)],[(260,400),(140,400)]])
            self.envWallSide = ['l', 't','r', 'b', 'r', 'b']           
        # this is a turn to the right which is high but not that high
        elif envNr == 6:
            tYs = 270 # rurnYstart This is the start of the right side
            tYe = 110 #turn Y end, the top of the pipe
            self.envWalls = np.array([[(140,400), (140,tYe)], [(140,tYe), (355,tYe)], [(355,tYe),
                      (355,tYs)], [(355,tYs),(260,tYs)],[(260,tYs),(260,400)],[(260,400),(140,400)]])
            self.envWallSide = ['l', 't','r', 'b', 'r', 'b']     
            # This is a turn to the right which is the same as the original pipe to the left
        elif envNr == 7:
            tYs = 300 # rurnYstart This is the start of the right side
            tYe = 140 #turn Y end, the top op the pipe
            self.envWalls = np.array([[(140,400), (140,tYe)], [(140,tYe), (355,tYe)], [(355,tYe),
                      (355,tYs)], [(355,tYs),(260,tYs)],[(260,tYs),(260,400)],[(260,400),(140,400)]])
            self.envWallSide = ['l', 't','r', 'b', 'r', 'b']     
        else:
            raise NameError('envNr was out of range, no such environment defined')



Uit constants class:






# -*- coding: utf-8 -*-
"""
Created on Tue Oct 23 12:28:34 2018

@author: arnol
"""

import numpy as np

ENV_IS_RARM = True

# PARS FOR EVALUATION ONLY
EVAL_MODE = False
EVAL_RENDER = True # only relevant ruing evaluation episodes
EVAL_SHOW_NORMAL_SPEED = True # only relevant during evaluation episodes
EVAL_FPS = 60 # only relevant during evalutation episodes
#EVAL_CPU_CNT = 12 # Number of cpu's used during training
EVAL_FOLDER = 'runLocal31oktNachtG98' # The folder that holds the model and train folders
# deze heb je nodig om iets te evalueren in een compleet andere map
#EVAL_HIGHLEVEL_FOLDER = './LogsOfRuns/TempTransferCluster29okt'
#tussen = EVAL_HIGHLEVEL_FOLDER + '/' + EVAL_FOLDER
#EVAL_MODEL_PATH = tussen + "/model"
EVAL_NR_OF_GAMES = 100
# ===================================================================

sim_WINDOW_WIDTH = 400
sim_WINDOW_HEIGHT = 400

sim_RandomGoal = True
sim_SparseRewards = False
sim_Goal = np.array([100,200])
sim_AddNoise = True
sim_goalRadius = 5
sim_GoalReward = 100.
sim_expRewardGamma = -0.01 
sim_expRewardOffset = 100
sim_thresholdWall = 20. # linear punishmet starts at this amount of pixels
sim_WallReward = 100
# the normalisation is dependend on whether sparse rewards are used
if sim_SparseRewards:
    # the higest reward must be 1, this ensures that is true.
    sim_rewardNormalisation = max(sim_GoalReward,sim_WallReward)
else: # in case no sparse rewards    
    sim_rewardNormalisation = sim_WallReward + sim_expRewardOffset
print("GlobalCOnst ",sim_rewardNormalisation)


rob_RandomInit = False
rob_RandomWalls = True
rob_NoiseStandDev = 0.001
rob_StepSize = np.radians(1.4)
rob_MaxJointAngle = np.radians(np.array([170,-170]))
rob_JointLenght = [100,100,80,20]#[10, 10, 5, 5] # 100,100,80,20
rob_JointWidth = 10
rob_ResetAngles = np.radians(np.array([115,-95,115])) # this is used with randomWalls = True
#rob_ResetAngles = np.radians(np.array([130,-140,140])) # this was used during the 29okt runs

run_Render = False
run_NumOfWorkers = 12
run_MaxEpisodeLenght = 2000
run_FPS = 15
run_Gamma = .98 # discount rate for advantage estimation and reward discounting
run_sSize = 17 # Observations are greyscale frames of 84 * 84 * 1
run_aSize = 6 # Agent can rotate each joint in 2 directions
run_LoadModel = False
run_LearningRate = 1e-3
run_epsilon = 0.1
run_decay = 0.99
run_BufferSize = 30
run_actionSpace = [1,2,3,4,5,6]
run_TFsummIntrvl = 5 # after this many episodes a datapoint is saved
run_TFmodelSaveIntrvl = 100 # after this many episodes the model is saved.

# Pars used in NETWORK
netw_nHidNodes = 512
netw_vfCoef = 0.5 # Was originally at 0.5, openAI also had 0.5
netw_entCoef = 0.1 # Was at 0.1 originally, openAI has 0.01 as default in A2C
netw_maxGradNorm = 40 # was originally 40 and openAI has 0.5 in A2C


OUTP_FOLDER = '/runLocal31oktNachtG98'

OUTP_FOLDER = './LogsOfRuns' + OUTP_FOLDER
TF_SUMM_PATH = OUTP_FOLDER + '/train_'
run_ModelPath = OUTP_FOLDER + '/model'





