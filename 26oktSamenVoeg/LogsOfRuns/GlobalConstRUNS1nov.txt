NOTE: Deze runs zijn gedaan met de 2 bugs erin dat ie door een hoek naar rechts heen kon, en dat de goal soms tegen de muur aan werd geplaatst.

	run01novInitLinksG995
Init op links, dus 115,-95,115 en gamma naar 995 om te testen of dit lukt om dan ver genoeg 
vooruit te kijken om een move naar rechts te maken.
	run01novRandomIRGRWG98
Kijken hoe het gaat met random alles, init, goal, walls (7envs)
Eerst met 98 voor vergelijking en daarna met hogere gamma
	run01novRandomIRGRWG99
gamma op 99
	run01novRandomIRGRWG993
gamma op 993
	run01novRandomIRGRWG995
gamma op 995
	run01novRandomIRGRWG997
wat denk je zelf

Uit sim file:

   def setRandomEnv(self):
        # get the random envNr
        envNr = np.random.randint(1, 7 + 1)
        #envNr = 7
        # Switch between environments per reset of the environment.
        # This is a pipe witch a corner to the left (most used during training. Our first env.)
        if envNr == 1:
            self.envWalls = np.array([[(120,400), (120,300)], [(120,300), (40,300)], [(40,300),
                      (40,140)], [(40,140), (280,140)], [(280,140), (280,400)], [(280,400),(120,400)]])
            self.envWallSide = ['l', 'b','l', 't', 'r', 'b']
        # A pipe witch is straight up and wide
        elif envNr == 2:
            rC = 200 # robotCenter, the x pos of the arm bottom
            pR = 80 # the width of the pipe
            self.envWalls = np.array([[(rC-pR,400), (rC-pR,100)], [(rC-pR,100), (rC+pR,100)], [(rC+pR,100),
                      (rC+pR,400)], [(rC+pR,400), (rC-pR,400)]])
            self.envWallSide = ['l', 't','r', 'b']
        # A pipe witch is straight up and narrower
        elif envNr == 3:
            rC = 200 # robotCenter, the x pos of the arm bottom
            pR = 70 # the width of the pipe
            self.envWalls = np.array([[(rC-pR,400), (rC-pR,100)], [(rC-pR,100), (rC+pR,100)], [(rC+pR,100),
                      (rC+pR,400)], [(rC+pR,400), (rC-pR,400)]])
            self.envWallSide = ['l', 't','r', 'b']
        # this is a T shaped pipe 
        elif envNr == 4:
            tYe = 130
            self.envWalls = np.array([[(140,400), (140,300)], [(140,300), (45,300)], [(45,300),
                      (45,tYe)], [(45,tYe), (355,tYe)], [(355,tYe), (355,300)], [(355,300),(260,300)],[(260,300),(260,400)],[(260,400),(140,400)]])
            self.envWallSide = ['l', 'b','l', 't', 'r', 'b','r','b']#            
        # 
        # this is a turn to the right Which is at the top of the reach of the arm
        elif envNr == 5:
            tYs = 200 # rurnYstart This is the start of the right side of the pipe
            tYe = 110 #turn Y end, the top op the pipe
            self.envWalls = np.array([[(140,400), (140,tYe)], [(140,tYe), (355,tYe)], [(355,tYe),
                      (355,tYs)], [(355,tYs),(260,tYs)],[(260,tYs),(260,400)],[(260,400),(140,400)]])
            self.envWallSide = ['l', 't','r', 'b', 'r', 'b']           
        # this is a turn to the right which is high but not that high
        elif envNr == 6:
            tYs = 270 # rurnYstart This is the start of the right side
            tYe = 110 #turn Y end, the top of the pipe
            self.envWalls = np.array([[(140,400), (140,tYe)], [(140,tYe), (355,tYe)], [(355,tYe),
                      (355,tYs)], [(355,tYs),(260,tYs)],[(260,tYs),(260,400)],[(260,400),(140,400)]])
            self.envWallSide = ['l', 't','r', 'b', 'r', 'b']     
            # This is a turn to the right which is the same as the original pipe to the left
        elif envNr == 7:
            tYs = 300 # rurnYstart This is the start of the right side
            tYe = 140 #turn Y end, the top op the pipe
            self.envWalls = np.array([[(140,400), (140,tYe)], [(140,tYe), (355,tYe)], [(355,tYe),
                      (355,tYs)], [(355,tYs),(260,tYs)],[(260,tYs),(260,400)],[(260,400),(140,400)]])
            self.envWallSide = ['l', 't','r', 'b', 'r', 'b']     
        else:
            raise NameError('envNr was out of range, no such environment defined')

-------------------------------------------------
Constants file:

# -*- coding: utf-8 -*-
"""
Created on Tue Oct 23 12:28:34 2018

@author: arnol
"""

import numpy as np

ENV_IS_RARM = True

# PARS FOR EVALUATION ONLY
EVAL_MODE = False
EVAL_RENDER = True # only relevant ruing evaluation episodes
EVAL_SHOW_NORMAL_SPEED = True # only relevant during evaluation episodes
EVAL_FPS = 60 # only relevant during evalutation episodes
#EVAL_CPU_CNT = 12 # Number of cpu's used during training
EVAL_FOLDER = 'run30oktLike29oktG99' # The folder that holds the model and train folders
# deze heb je nodig om iets te evalueren in een compleet andere map
#EVAL_HIGHLEVEL_FOLDER = './LogsOfRuns/TempTransferCluster29okt'
#tussen = EVAL_HIGHLEVEL_FOLDER + '/' + EVAL_FOLDER
#EVAL_MODEL_PATH = tussen + "/model"
EVAL_NR_OF_GAMES = 100
# ===================================================================

sim_WINDOW_WIDTH = 400
sim_WINDOW_HEIGHT = 400

sim_RandomGoal = True
sim_SparseRewards = False
sim_Goal = np.array([100,200])
sim_AddNoise = True
sim_goalRadius = 5
sim_GoalReward = 100.
sim_expRewardGamma = -0.01 
sim_expRewardOffset = 100
sim_thresholdWall = 20. # linear punishmet starts at this amount of pixels
sim_WallReward = 100
# the normalisation is dependend on whether sparse rewards are used
if sim_SparseRewards:
    # the higest reward must be 1, this ensures that is true.
    sim_rewardNormalisation = max(sim_GoalReward,sim_WallReward)
else: # in case no sparse rewards    
    sim_rewardNormalisation = sim_WallReward + sim_expRewardOffset
print("GlobalCOnst ",sim_rewardNormalisation)


rob_RandomInit = False
rob_RandomWalls = True
rob_NoiseStandDev = 0.001
rob_StepSize = np.radians(1.4)
rob_MaxJointAngle = np.radians(np.array([170,-170]))
rob_JointLenght = [100,100,80,20]#[10, 10, 5, 5] # 100,100,80,20
rob_JointWidth = 10
#rob_ResetAngles = np.radians(np.array([65,115,-115])) # gericht op spul naar rechts
rob_ResetAngles = np.radians(np.array([115,-95,115])) # this is used with randomWalls = True in 31 okt runs (gericht op spul naar links)
#rob_ResetAngles = np.radians(np.array([130,-140,140])) # this was used during the 29okt runs

run_Render = False
run_NumOfWorkers = 12
run_MaxEpisodeLenght = 2000
run_FPS = 15
run_Gamma = .995 # discount rate for advantage estimation and reward discounting
run_sSize = 17 # Observations are greyscale frames of 84 * 84 * 1
run_aSize = 6 # Agent can rotate each joint in 2 directions
run_LoadModel = False
run_LearningRate = 1e-3
run_epsilon = 0.1
run_decay = 0.99
run_BufferSize = 30
run_actionSpace = [1,2,3,4,5,6]
run_TFsummIntrvl = 5 # after this many episodes a datapoint is saved
run_TFmodelSaveIntrvl = 100 # after this many episodes the model is saved.

# Pars used in NETWORK
netw_nHidNodes = 512
netw_vfCoef = 0.5 # Was originally at 0.5, openAI also had 0.5
netw_entCoef = 0.1 # Was at 0.1 originally, openAI has 0.01 as default in A2C
netw_maxGradNorm = 40 # was originally 40 and openAI has 0.5 in A2C


OUTP_FOLDER = '/run01novInitLinksG995'

OUTP_FOLDER = './LogsOfRuns' + OUTP_FOLDER
TF_SUMM_PATH = OUTP_FOLDER + '/train_'
run_ModelPath = OUTP_FOLDER + '/model'









